{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "################################################################################\n",
    "#Licensed Materials - Property of IBM\n",
    "#(C) Copyright IBM Corp. 2019\n",
    "#US Government Users Restricted Rights - Use, duplication disclosure restricted\n",
    "#by GSA ADP Schedule Contract with IBM Corp.\n",
    "################################################################################\n",
    "\n",
    "The auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for Watson Studio Auto-generated Notebook (\"License Terms\"), such agreements located in the link below.\n",
    "Specifically, the Source Components and Sample Materials clause included in the License Information document for\n",
    "Watson Studio Auto-generated Notebook applies to the auto-generated notebooks. \n",
    "By downloading, copying, accessing, or otherwise using the materials, you agree to the License Terms.\n",
    "http://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BHU2B7&title=IBM%20Watson%20Studio%20Auto-generated%20Notebook%20V2.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM AutoAI Auto-Generated Notebook v1.10.6\n",
    "### Representing Pipeline: P4 from run b3dd1f4e-8d60-4908-a0d0-787e8bc28e16\n",
    "\n",
    "**Note**: Notebook code generated using AutoAI will execute successfully.  If code is modified or reordered, there is no guarantee it will successfully execute, please read our documentation for more information https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/autoai-notebook.html .\n",
    "\n",
    "Before modifying the pipeline or trying to re-fit the pipeline, consider:\n",
    "The notebook converts dataframes to numpy arrays before fitting the pipeline (a current restriction of the preprocessor pipeline).\n",
    "The known_values_list is passed by reference and populated with categorical values during fit of the preprocessing pipeline.  Delete its members before re-fitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "try:\n",
    "    import xgboost\n",
    "except:\n",
    "    print('xgboost, if needed, will be installed and imported later')\n",
    "try:\n",
    "    import lightgbm\n",
    "except:\n",
    "    print('lightgbm, if needed, will be installed and imported later')\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "import numpy\n",
    "from numpy import nan, dtype, mean\n",
    "import autoai_libs\n",
    "from autoai_libs.sklearn.custom_scorers import CustomScorers\n",
    "import sklearn.ensemble\n",
    "from autoai_libs.cognito.transforms.transform_utils import TExtras, FC\n",
    "from autoai_libs.transformers.exportable import *\n",
    "from autoai_libs.utils.exportable_utils import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "known_values_list=[]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compose Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "#THIS CELL HAS hmac CREDENTIALS TO READ YOUR PROJECT'S CLOUD OBJECT STORAGE BUCKET.  CONSIDER DELETING THE KEYS PRIOR TO SHARING THE NOTEBOOK.\n",
    "#Load pipeline while attempting to automatically import modules and install missing packages as necessary.\n",
    "retries = 0\n",
    "successful = False\n",
    "failed_retries = 0\n",
    "while retries < 100 and failed_retries < 10 and not successful:\n",
    "    retries += 1\n",
    "    failed_retries += 1\n",
    "    try:\n",
    "        #\n",
    "        # composing steps for toplevel Pipeline\n",
    "        #\n",
    "        _input_metadata = {'run_uid': 'b3dd1f4e-8d60-4908-a0d0-787e8bc28e16', 'pn': 'P4', 'data_source': '', 'target_label_name': 'Fraud_Risk', 'learning_type': 'classification', 'optimization_metric': 'roc_auc', 'random_state': 33, 'cv_num_folds': 3, 'holdout_fraction': 0.1, 'data_provenance': {'input_data': [{'connection': {'access_key_id': '0b2c0315866241c19aea1060b7b555e7', 'secret_access_key': 'be6153e5e8bca24e81c538e437781b0260cb6bd64417502d', 'endpoint_url': 'https://s3-api.us-geo.objectstorage.softlayer.net'}, 'id': '1', 'location': {'type': 's3', 'bucket': 'ibmwatsonstudioampopenscaleworksh-donotdelete-pr-zabaxvri8uoz4w', 'path': 'fraud_dataset.csv'}, 'type': 's3'}]}, 'pos_label': 1.0}\n",
    "        steps=[]\n",
    "        #\n",
    "        # composing steps for preprocessor Pipeline\n",
    "        #\n",
    "        preprocessor__input_metadata = None\n",
    "        preprocessor_steps=[]\n",
    "        #\n",
    "        # composing steps for preprocessor_features FeatureUnion\n",
    "        #\n",
    "        preprocessor_features_transformer_list=[]\n",
    "        #\n",
    "        # composing steps for preprocessor_features_categorical Pipeline\n",
    "        #\n",
    "        preprocessor_features_categorical__input_metadata = None\n",
    "        preprocessor_features_categorical_steps=[]\n",
    "        preprocessor_features_categorical_steps.append(('cat_column_selector', autoai_libs.transformers.exportable.NumpyColumnSelector(columns=[0, 1, 2, 3, 4, 8, 9, 10, 11])))\n",
    "        preprocessor_features_categorical_steps.append(('cat_compress_strings', autoai_libs.transformers.exportable.CompressStrings(activate_flag=True, compress_type='hash',\n",
    "                dtypes_list=['int_num', 'int_num', 'int_num', 'int_num', 'int_num', 'int_num', 'int_num', 'int_num', 'int_num'],\n",
    "                missing_values_reference_list=['', '-', '?', nan],\n",
    "                misslist_list=[[], [], [], [], [], [], [], [], []])))\n",
    "        preprocessor_features_categorical_steps.append(('cat_missing_replacer', autoai_libs.transformers.exportable.NumpyReplaceMissingValues(filling_values=nan, missing_values=[])))\n",
    "        preprocessor_features_categorical_steps.append(('cat_unknown_replacer', autoai_libs.transformers.exportable.NumpyReplaceUnknownValues(filling_values=nan,\n",
    "                     filling_values_list=[nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
    "                     known_values_list=known_values_list,\n",
    "                     missing_values_reference_list=['', '-', '?', nan])))\n",
    "        preprocessor_features_categorical_steps.append(('boolean2float_transformer', autoai_libs.transformers.exportable.boolean2float(activate_flag=True)))\n",
    "        preprocessor_features_categorical_steps.append(('cat_imputer', autoai_libs.transformers.exportable.CatImputer(activate_flag=True, missing_values=nan,\n",
    "              sklearn_version_family='20', strategy='most_frequent')))\n",
    "        preprocessor_features_categorical_steps.append(('cat_encoder', autoai_libs.transformers.exportable.CatEncoder(activate_flag=True, categories='auto',\n",
    "              dtype=numpy.float64, encoding='ordinal',\n",
    "              handle_unknown='error', sklearn_version_family='20')))\n",
    "        preprocessor_features_categorical_steps.append(('float32_transformer', autoai_libs.transformers.exportable.float32_transform(activate_flag=True)))\n",
    "        # assembling preprocessor_features_categorical_ Pipeline\n",
    "        preprocessor_features_categorical_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_features_categorical_steps)\n",
    "        preprocessor_features_transformer_list.append(('categorical', preprocessor_features_categorical_pipeline))\n",
    "        #\n",
    "        # composing steps for preprocessor_features_numeric Pipeline\n",
    "        #\n",
    "        preprocessor_features_numeric__input_metadata = None\n",
    "        preprocessor_features_numeric_steps=[]\n",
    "        preprocessor_features_numeric_steps.append(('num_column_selector', autoai_libs.transformers.exportable.NumpyColumnSelector(columns=[5, 6, 7])))\n",
    "        preprocessor_features_numeric_steps.append(('num_floatstr2float_transformer', autoai_libs.transformers.exportable.FloatStr2Float(activate_flag=True,\n",
    "                dtypes_list=['int_num', 'int_num', 'int_num'],\n",
    "                missing_values_reference_list=[])))\n",
    "        preprocessor_features_numeric_steps.append(('num_missing_replacer', autoai_libs.transformers.exportable.NumpyReplaceMissingValues(filling_values=nan, missing_values=[])))\n",
    "        preprocessor_features_numeric_steps.append(('num_imputer', autoai_libs.transformers.exportable.NumImputer(activate_flag=True, missing_values=nan, strategy='median')))\n",
    "        preprocessor_features_numeric_steps.append(('num_scaler', autoai_libs.transformers.exportable.OptStandardScaler(num_scaler_copy=None, num_scaler_with_mean=None,\n",
    "                 num_scaler_with_std=None, use_scaler_flag=False)))\n",
    "        preprocessor_features_numeric_steps.append(('float32_transformer', autoai_libs.transformers.exportable.float32_transform(activate_flag=True)))\n",
    "        # assembling preprocessor_features_numeric_ Pipeline\n",
    "        preprocessor_features_numeric_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_features_numeric_steps)\n",
    "        preprocessor_features_transformer_list.append(('numeric', preprocessor_features_numeric_pipeline))\n",
    "        # assembling preprocessor_features_ FeatureUnion\n",
    "        preprocessor_features_pipeline = sklearn.pipeline.FeatureUnion(transformer_list=preprocessor_features_transformer_list)\n",
    "        preprocessor_steps.append(('features', preprocessor_features_pipeline))\n",
    "        preprocessor_steps.append(('permuter', autoai_libs.transformers.exportable.NumpyPermuteArray(axis=0,\n",
    "                 permutation_indices=[0, 1, 2, 3, 4, 8, 9, 10, 11, 5, 6, 7])))\n",
    "        # assembling preprocessor_ Pipeline\n",
    "        preprocessor_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_steps)\n",
    "        steps.append(('preprocessor', preprocessor_pipeline))\n",
    "        #\n",
    "        # composing steps for cognito Pipeline\n",
    "        #\n",
    "        cognito__input_metadata = None\n",
    "        cognito_steps=[]\n",
    "        cognito_steps.append(('0', autoai_libs.cognito.transforms.transform_utils.TA1(fun = numpy.square, name = 'square', datatypes = ['numeric'], feat_constraints = [FC.is_not_categorical], tgraph = None, apply_all = True, col_names = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Term', 'Credit_History_Available', 'Housing', 'Locality'], col_dtypes = [dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32')])))\n",
    "        cognito_steps.append(('1', autoai_libs.cognito.transforms.transform_utils.FS1(cols_ids_must_keep = range(0, 12), additional_col_count_to_keep = 12, ptype = 'classification')))\n",
    "        cognito_steps.append(('2', autoai_libs.cognito.transforms.transform_utils.TAM(tans_class=sklearn.decomposition.pca.PCA(copy = True, iterated_power = 'auto', n_components = None, random_state = None, svd_solver = 'auto', tol = 0.0, whiten = False), name = 'pca', tgraph = None, apply_all = True, col_names = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Term', 'Credit_History_Available', 'Housing', 'Locality', 'square(ApplicantIncome)', 'square(CoapplicantIncome)', 'square(LoanAmount)', 'square(Loan_Term)'], col_dtypes = [dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32')])))\n",
    "        cognito_steps.append(('3', autoai_libs.cognito.transforms.transform_utils.FS1(cols_ids_must_keep = range(0, 12), additional_col_count_to_keep = 12, ptype = 'classification')))\n",
    "        # assembling cognito_ Pipeline\n",
    "        cognito_pipeline = sklearn.pipeline.Pipeline(steps=cognito_steps)\n",
    "        steps.append(('cognito', cognito_pipeline))\n",
    "        steps.append(('estimator', sklearn.ensemble.forest.RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
    "                    criterion='gini', max_depth=5, max_features=1.0,\n",
    "                    max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                    min_impurity_split=None, min_samples_leaf=1,\n",
    "                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                    n_estimators=90, n_jobs=4, oob_score=True, random_state=33,\n",
    "                    verbose=0, warm_start=False)))\n",
    "        # assembling  Pipeline\n",
    "        pipeline = sklearn.pipeline.Pipeline(steps=steps)\n",
    "\n",
    "        successful = True\n",
    "    except Exception as e:\n",
    "        estr = str(e)\n",
    "        if estr.startswith('name ') and estr.endswith(' is not defined'):\n",
    "            try:\n",
    "                import importlib\n",
    "                module_name = estr.split(\"'\")[1]\n",
    "                module = importlib.import_module(module_name)\n",
    "                globals().update({module_name: module})\n",
    "                print('import successful for ' + module_name)\n",
    "                failed_retries -= 1\n",
    "            except Exception as import_failure:\n",
    "                print('import of ' + module_name + ' failed with: ' + str(import_failure))\n",
    "                import subprocess\n",
    "                print('attempting pip install of ' + module_name)\n",
    "                process = subprocess.Popen('pip install ' + module_name, shell=True)\n",
    "                process.wait()\n",
    "                try:\n",
    "                    print('re-attempting import of ' + module_name)\n",
    "                    module = importlib.import_module(module_name)\n",
    "                    globals().update({module_name: module})\n",
    "                    print('import successful for ' + module_name)\n",
    "                    failed_retries -= 1\n",
    "                except Exception as import_or_installation_failure:\n",
    "                    print('failure installing and/or importing ' + module_name + ' error was: ' + str(\n",
    "                        import_or_installation_failure))\n",
    "                    raise (ModuleNotFoundError('Missing package in environment for ' + module_name +\n",
    "                        '? Try import and/or pip install manually?'))\n",
    "        elif type(e) is AttributeError:\n",
    "            if 'module ' in estr and ' has no attribute ' in estr:\n",
    "                pieces = estr.split(\"'\")\n",
    "                if len(pieces) == 5:\n",
    "                    try:\n",
    "                        import importlib\n",
    "                        print('re-attempting import of ' + pieces[3] + ' from ' + pieces[1])\n",
    "                        module = importlib.import_module('.' + pieces[3], pieces[1])\n",
    "                        failed_retries -= 1\n",
    "                    except:\n",
    "                        print('failed attempt to import ' + pieces[3])\n",
    "                        raise(e)\n",
    "                else:\n",
    "                    raise(e)\n",
    "        else:\n",
    "            raise(e)\n",
    "if successful:\n",
    "    print('Pipeline successfully instantiated')\n",
    "else:\n",
    "    raise (ModuleNotFoundError('Remaining missing imports/packages in environment? Retry cell and/or try pip install manually?'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Metadata used in retrieving data and computing metrics.  Customize as necessary for your environment.\n",
    "data_source='replace_with_path_and_csv_filename'\n",
    "target_label_name = _input_metadata['target_label_name']\n",
    "learning_type = _input_metadata['learning_type']\n",
    "optimization_metric = _input_metadata['optimization_metric']\n",
    "random_state = _input_metadata['random_state']\n",
    "cv_num_folds = _input_metadata['cv_num_folds']\n",
    "holdout_fraction = _input_metadata['holdout_fraction']\n",
    "data_provenance = _input_metadata['data_provenance']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csv_encodings=['UTF-8','Latin-1'] # supplement list of encodings as necessary for your data\n",
    "df = None\n",
    "if type(data_provenance) is str: # try to load file specified\n",
    "    for encoding in csv_encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(data_provenance, encoding=encoding) # your data file name here\n",
    "        except Exception as csv_exception:\n",
    "            print(csv_exception)\n",
    "if df is None: # try to load file from Cloud Object Storage\n",
    "    try:\n",
    "        data_location = data_provenance['input_data'][0]\n",
    "        print('data_location '+ str(data_location))\n",
    "        import boto3\n",
    "        session = boto3.session.Session()\n",
    "        cos = session.client(\n",
    "            service_name='s3',\n",
    "            aws_access_key_id=data_location['connection']['access_key_id'],\n",
    "            aws_secret_access_key=data_location['connection']['secret_access_key'],\n",
    "            endpoint_url=data_location['connection']['endpoint_url'],\n",
    "            verify=False\n",
    "        )\n",
    "        local_path = data_location['location']['path']\n",
    "        print('local_path ' + str(local_path))\n",
    "        cos.download_file(data_location['location']['bucket'],\n",
    "                     data_location['location']['path'],\n",
    "                     local_path)\n",
    "        for encoding in csv_encodings:\n",
    "            try:\n",
    "                df = pd.read_csv(local_path, encoding=encoding) # your data file name here\n",
    "            except Exception as csv_exception:\n",
    "                print(csv_exception)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "if df is None:\n",
    "    raise(ValueError('Problem accessing or decoding csv data. May need csv_encoding string, or location or credential information to read dataframe from COS'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = target_label_name # your target name here\n",
    "df_y = df[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df.drop(columns=[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if learning_type is None:\n",
    "    # When the problem type is not available in the metadata, use the sklearn type_of_target to determine whether to stratify the holdout split\n",
    "    # Caution:  This can mis-classify regression targets that can be expressed as integers as multiclass, in which case manually override the learning_type\n",
    "    from sklearn.utils.multiclass import type_of_target\n",
    "    if type_of_target(df_y.values) in ['multiclass', 'binary']:\n",
    "        learning_type = 'classification'\n",
    "    else:\n",
    "        learning_type = 'regression'\n",
    "    print('learning_type determined by type_of_target as:',learning_type)\n",
    "else:\n",
    "    print('learning_type specified as:',learning_type)\n",
    "if learning_type == 'classification':\n",
    "    X, X_holdout, y, y_holdout = train_test_split(df_X.values, df_y.values, test_size=holdout_fraction, random_state=random_state, stratify=df_y.values)\n",
    "else:\n",
    "    X, X_holdout, y, y_holdout = train_test_split(df_X.values, df_y.values, test_size=holdout_fraction, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_holdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_holdout)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import get_scorer\n",
    "scorer = get_scorer(optimization_metric)\n",
    "scorer(pipeline, X_holdout, y_holdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete CONTENTS of known_values_list before refitting, cloning or cross_validate-ing the pipeline, or previous values will be used.\n",
    "for i in range(len(known_values_list)):\n",
    "    del known_values_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor should see all data for cross_validate on the remaining steps to match autoai scores\n",
    "preprocessor = pipeline.steps[0][1]\n",
    "preprocessor.fit(X)\n",
    "X_prep = preprocessor.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove preprocessor from pipeline\n",
    "del pipeline.steps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_validate on remaining steps of pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "cv_results = cross_validate(pipeline, X_prep, y, scoring={optimization_metric:scorer})\n",
    "import numpy as np\n",
    "np.mean(cv_results['test_' + optimization_metric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
